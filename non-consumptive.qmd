---
title: "Sharing *is* Caring (about Research): Four Avenues for Sharing Text Collections and the Need for Non-Consumptive Research"
author:
  - name: "Johannes B. Gruber"
    affiliations:
      - "Vrije Universiteit Amsterdam"
      - name: "University of Amsterdam"
  - name: "Wouter van Atteveldt"
    affiliations:
      - "Vrije Universiteit Amsterdam"
  - name: "Kasper Welbers"
    affiliations:
      - "Vrije Universiteit Amsterdam"
format:
  ccr-pdf:
    keep-tex: true
    citeproc: false
    code-overflow: wrap
    filters: [_extensions/citeproc.lua, _extensions/wordcount.lua]
    include-in-header:
      - text: |
          \AddToHook{env/Highlighting/begin}{\scriptsize}
pdf-engine: pdflatex
keywords: Non-Consumptive Research, computational methods
volume: 1
pubnumber: 1
pubyear: 2023
firstpage: 1
shortauthors: One & Two
shorttitle: "Sharing *is* Caring (about Research)"
bibliography: bibliography.bib
execute:
  eval: false
code-line-numbers: true
abstract: |
  This paper urges you to share your data and presents the The Amsterdam Content Analysis Toolkit, which can be used to share your copora as widely as possible while respecting legal and ethical limitations.
---

Text-as-data has arrived as a prominent method for content analysis in the social sciences.
The twin promises of making large scale analysis of text corpora feasible while keeping the costs for manual annotation to a minimum convinced institutions and individual researchers to make considerable investments into methods training and embrace the computational revolution of communication science in the last decade \[@grimmerTADA2013; (e.g., see overview articles of Brady, 2019; Hilbert et al., 2019; Lazer & Radford, 2017; van Atteveldt & Peng, 2018; Van Atteveldt et al., 2019)\].
However, since the early days, a lot of the hopes and enthusiasm has evaporated as access to the data gold mines of social media databases and digital news archives are progressively limited by their owners.
And more importantly, pressing questions about political communication stay untackled, as researchers are shut out.

Just this year, Twitter, which was renamed to $\mathbb{X}$ recently, eliminated the free academic access to their API^[
<https://web.archive.org/web/20230831000123/https://www.theverge.com/2023/3/30/23662832/twitter-api-tiers-free-bot-novelty-accounts-basic-enterprice-monthly-price>
] and Reddit has shut down access to its API^[
<https://web.archive.org/web/20230829045754/https://www.reddit.com/r/modnews/comments/134tjpe/reddit_data_api_update_changes_to_pushshift_access/>
] for Pushshift, a service on which most academic research of that social media site relied.
Meanwhile Meta's tool to gather data from Facebook and Instagram, Crowdtangle, has been publicly rumored to be closing down for several years^[
<https://www.bloomberg.com/news/articles/2022-06-23/meta-pulls-support-for-tool-used-to-keep-misinformation-in-check>;
<https://web.archive.org/web/20230816171112/https://www.theverge.com/2022/6/23/23180357/meta-crowdtangle-shut-down-facebook-misinformation-viral-news-tracker>;
<https://web.archive.org/web/20230827165945/https://www.abc.net.au/news/science/2022-08-16/facebook-crowdtangle-meta-disinformation-transparency/101325544>
], making it difficult to plan, or request funding for, any projects relying on this data.
Likewise, digital news archives, like LexisNexis, increasingly guard their sites against large scale data collection or make it out of reach expensive.
Meanwhile, newly developed or strengthened privacy and ethical standards and laws -- while both important and welcome -- further limit access to data for analysis.
The Post-API Age, which Freelon predicted in 2018, seems to have reached a new stage and the promised gold mines of digital communication traces are no longer accessible.

In sum, this limited data availability also broadens the gap between academics once more.
While computational methods have decreased the funding needs for analysis, limited availability of data leads to new divides in the field between those who can afford to negotiate access, or have industry ties that enable them to collaborate with social media companies, and those who cannot.
Likewise, web scraping content, as @freelonPostAPI2018 suggests, requires know-how and resources not available to all researchers.
Especially as more platforms, outlets and individuals try to block their content from being scraped by AI companies -- and de facto everyone else.^[
<https://www.nytimes.com/2023/07/15/technology/artificial-intelligence-models-chat-data.html>]
These problems have been recognised in the field and are discussed as important challenges at methodological and issue specific conferences^[
Like the keynote roundtable at Comptext 2023 or the *The Post-API Conference* Annenberg Public Policy Center in 2023.
] as well as in specialized working groups like the EDMO Working Group on Access to Platform Data^[
<https://web.archive.org/web/20230606203119/https://edmo.eu/2021/08/30/launch-of-the-edmo-working-group-on-access-to-platform-data/>
].

In this contribution, we thus want to highlight a mitigation strategy that grows in importance given the narrowing ways to access communication data:
sharing already gathered corpora for secondary analysis.
For data collected through, e.g., surveys, this strategy has been established for decades.
For text corpora, there are often legal or ethical barriers to what can be shared.
We thus present four avenues to share data sets, structured by what is legally and morally justifiable in light of the interests of data owners.
We put a particular focus on the possibility of making data available for *non-consumptive research*, in cases where other strategies appear impossible.
Some of the options we discuss rely on infrastructure that currently does not exist, like severs to host tools for *non-consumptive research*.
We thus additionally offer an open source software toolkit which allows universities, other institutions, or even individual researchers to enable almost anyone to share at least a portion of their collected data online, while respecting legal and ethical limitations to what can and cannot be shared.
In particular, we developed this toolkit based on a modular design and open source frameworks, focused on the needs we identified for ourselves and the community. 


# Four Avenues for Sharing Text Collections
## Make the entire dataset available

The first avenue is obvious:
make data sets available for secondary research where possible.
This can enable others to quickly build on work after the original data set has been gathered.
It saves researchers from duplicating work and enables them to focus their energy on answering important questions for society that might otherwise stay uncovered.
What might be less obvious is why those who gathered the data in the first place might want to do that.
We believe that there are at least three convincing arguments why one should consider the step:

1.  **Lend your research credibility by making it reproducible:** One of the cornerstones of robust scientific research is reproducibility, that is that others than the original researcher can take the original data and methods to  reproduce the findings of a study [@barba2018terminologies]. When the original data is not available, reproducibility is impossible by default. By making data sets available, original researchers expose themselves to more rigor, which lends transparency and credibility to the original research, but also strengthens the foundation of scientific inquiry as a whole.

2. **Gather citations (beyond the own field):** data sets that are available and are used for secondary analysis are usually also cited in new studies. Given the importance for citation counts in most academic systems worldwide, this is a big incentive for those who spent the resources to gather the original data. What researchers should also keep in mind here is that the impact of the data set might well end up surpassing that of the original research, given that data collections might attract researchers from other disciplines than their own. As computer and data scientists become more interested in social science questions, gathered and annotated data has become a valuable resource in these fields, for example.

3. **Promote the original research:** Sharing data also acts as a promotional tool. When other researchers access and use this data, they will almost always become aware of the original research and build on it. It then often makes sense for them not to build upon the original findings and further promote the findings through their own contribution.

On the downside, making a data set publicly available, however, will also often mean that researchers need to check with data owners if sharing data sets is allowed.
Gathering data that is publicly available for academic research is usually considered except from copyright claims by fair use factors.
However, sharing a collected corpus usually constitutes a pure duplication of copyright-protected, which is not covered by fair use [@HennesyLaw2019].
If data has been bought or has been obtained through an agreement with the data owner(s), they might also be opposed to redistribution or demand additional reimbursement, making it necessary to request additional funding.
However, making data openly accessible promotes scientific collaboration, enables the reproducibility of results, and fosters the advancement of knowledge.
When researchers can access and build upon one another's data, it accelerates scientific progress and helps to avoid duplicative efforts.
Useful text collections will also be referenced repeatedly, providing additional recognition for those who collected the data.
All of this hopefully convinces research teams and funders that the additional efforts are well worth it.

## Making pre-processed versions of texts available 

Many researchers might be aware of the advantages that data sharing have for the research community and themselves, but refrain from it anyway, given obstacles and dangers involved with sharing them.
However, the legal and ethical limitations to free access to information where they infringe on copyrights or on the privacy of data owners might not in all cases extend to their pre-processed versions.
Pre-processing of textual data can be roughly divided into *destructive* and *non-destructive* steps.
In the best case scenario, data owners would give permission to make a tokenized version of their text -- that is, a text split into its words and punctuation characters in their original order -- available.
Other researchers can then rebuild the data for all the same analyses that can be performed on the original set.
The caveat of such non-destructive pre-processing, however, is that the same restrictions that apply on the original data likely also apply to the pre-processed data.

Where such restrictions are a concern, *destructive* pre-processing steps could instead be performed before sharing textual data.
This could involve turning text into a document-feature-matrix -- that is a table which contains counts of how often each word, punctuation character or symbol occurs in a text.
In these cases bag-of-word approaches can still be employed to analyze the data, but the original texts can usually not be recreated^[
If original texts are very short, it might still be possible to some degree.
].
For most limitations regarding copyright laws, making data available in such a format constitutes fair use "the effect of the use upon the potential market for or value of the copyrighted work" [@HennesyLaw2019] is essentially non-existent.
Essentially, someone who wanted to read an article or book would gain little from staring at a document-feature-matrix, leaving the data owner's chances to make money from selling the work untouched.
In fact, the platform [Media Cloud](www.mediacloud.org) has practiced this avenue for a while to share web-scraped content from hundreds of online news sites without known issues.
In cases where it is crucial to protect someone's privacy this might not be enough, however.
Additional steps could be applied to create a privatized version of the data, for example, by replacing each feature description of a document-feature-matrix with a synonym.
This decreases the usefulness of the data somewhat, but it can still be used to reproduce findings or improve upon original models.

A second destructive pre-processing step that has gained popularity in recent years is to encode texts into embeddings.
Word or text embeddings produced through models like BERT [@devlinBERT2019] are a common transformation process in computational text analysis now and are usually employed before a classification model is trained [e.g., @theocharis:2020:DPI; @rodriguezEmbedding2023; @laurerBERT-NLI2023; @SimonTelegramsphere2022].
This means that sharing the encoded data, along with the original training and test data, would allow other researchers to improve upon the original model and use it in similar contexts.
In these cases as well, the original texts can not be recovered, evading most concerns for data owners.
It also makes interpreting results harder and removes the possibility to add genuinely new annotations.
Nevertheless, many analysis methods remain possible, meaning that the presence of these datasets might still turn out to be invaluable for the research community.

## Make Metadata Available for Reconstruction of the Data

Whether sharing pre-processed versions of texts is possible or not, making metadata available is another important avenue to sharing corpora, either in addition or as an alternative.
The most important use of this metadata is that it can often be used to reconstruct the original data with far fewer resources or make the possibility available where the original data could otherwise not be gathered.
Referencing URLs, status IDs or, where not available, the dates of publication, headlines and authors of text, can enable others to collect the same data once more.
This might not seem important at first, but consider how time-consuming and difficult it often is to identify the relevant portion of a population text.
At the minimum, a database must be queried using carefully selected keywords, before validating the obtained corpus to make sure false positives are eliminated [@King.2017].
Skipping this step in secondary analysis of data means that it can proceed more quickly, freeing up resources to focus on important substantive research questions.

In other cases, obtaining a selection of relevant texts might be impossible.
As many platforms and outlets do not offer a way to query the content or demand additional compensation, using keywords to select relevant portions of a database might be impossible.
However, gathering all content to filter afterwards is often infeasible as well as it might conflict with rate limits and/or require enormous resources for download and storage.
In sum, this can make a curated list of items the most valuable part of a shared dataset.

So while obtaining the full text still involves some extra efforts and challenges, if metadata of the original data is available, this enables researchers to reproduce (often in a limited fashion as not all items might still be available) and extend the original analysis.
In the case of Twitter data collections, the practice of sharing status IDs had become common and helped researchers to get around API limits as they were able to 'rehydrate' databases easily.^[
http://dfreelon.org/2012/02/11/arab-spring-twitter-data-now-available-sort-of/
] A practice that has unfortunately also become punishingly expensive under the new API rules.
A more encouraging example is again the platform [Media Cloud](www.mediacloud.org) from which researchers can obtain curated lists of URLs to news stories about specific topics.
Using tools such as news-please^[
https://github.com/fhamborg/news-please
] or paperboy^[
https://github.com/JBGruber/paperboy
], one can then quickly extract the full text, if the websites are still available.

## Make Non-consumptive Research Capabilities Available

A final avenue for making datasets available for secondary analysis is that of *non-consumptive research*.
Non-consumptive research means that a researcher analyzes texts without consuming, that is reading, them.
The term was popularized by two court cases which also exemplify its meaning:
that of Authors Guild v. HathiTrust, 755 F.3d 87 (2d Cir. 2014) and Authors Guild v. Google 721 F.3d 132 (2nd Cir. 2015).
In essence, both cases were about sharing tools to search copyrighted texts en masse to make counts, trends in usage and page numbers available for users.
In the case of *Google Books* the service's function to display text snippets surrounding a keyword hit was also reviewed by the court and found to be covered by fair use exceptions to copyright:

> "*The creation of a full-text searchable database is a quintessentially transformative use \[...\] \[T\]he result of a word search is different in purpose, character, expression, meaning, and message from the page (and the book) from which it is drawn.*" [quoted from @HennesyLaw2019, p. 294].

Transformative use here refers to the first factor that determines fair use:
if a work that uses copyrighted material adds something new, "with a further purpose or different character, and do not substitute for the original use of the work" (17 U.S.C.
§ 107).

While *non-consumptive research* was made popular through these mostly legal arguments to receive exceptions from copyright claims, the principles of *non-consumptive research* apply for most other texts that have limitations to free access, for example where they infringe on the privacy of data owners.
If the original text is not available to third parties, harm to those whose data is being analyzed is unlikely.
And to go even a step further: if the original content is not consumed by the primary researchers, potential harm to participants and the researchers is minimized.

The model of *non-consumptive research* can thus be employed and extended for most data collections where concerns or limitations against publications exist.
In Addition to allowing users to search the database and show which documents fit a query, aggregates (how many documents fit the query) and comparisons (e.g., do more speeches held by members of one party fit a query than those of another) are possible.
In this way, researchers might assess the relevance of a data collection for their own research questions before engaging with data owners to ask for permission or collecting the data again themselves.

Beyond the simple functions most people know from Google Books though, the definition of *non-consumptive research* is broader and comprises all computational analysis in which a researcher does not display substantial portions of the original text or image [@hathitrust2023].
Based on this, HathiTrust and others have experimented with "data capsules".
HathiTrust describes a capsule as a "system that grants a user access to a virtual machine which is a dedicated, secure desktop environment [...] through which a user can carry out non-consumptive research", yet which does not give researchers the ability to display or download the data [@hathitrust2023].
Given such a system, a wide range of unsupervised methods and destructive pre-processing steps become available to 

# Sharing Corpora with AmCAT

The Amsterdam Content Analysis Toolkit (AmCAT) has been in development in various guises since about 2001 as a text research / content analysis platform.
The core of the project has always been a database of documents, combined with graphical user interface and API to make it possible for casual and power users respectively to quickly gather the texts they need.
In the current fourth iteration of the toolkit, a particular focus was put on sharing data with an audience as broad as possible, but as narrow as legal and ethical circumstances allow.
It comes at the right time for researchers who want to take one of the avenues described above to share their corpora, but so far miss the infrastructure for it.

Besides providing a new dashboard and API wrappers for `R` and `Python` -- which make them great for working with collected corpora, or selecting a relevant subset -- one of the biggest improvements of AmCAT 4.0 is that it enables and facilitates the access and processing of data that cannot be shared openly by keeping data owners in direct control of the data, and by employing trusted connections and role-based access control methods.
Below, we describe some of the functions of the flexible framework access control and guest access and the (pre-processing) actions which extend AmCAT to be a toolkit for *non-consumptive research*.
Besides querying a database to get counts and trends of about the appearance of certain features, we developed a new framework, which we called `actioncat`, that can be used, for example, for pre-processing actions that allow users without access to the original data to decide which pre-processed versions of texts is then made available.
The software mentioned can be used for free and deployed by most users with a basic to advanced technical background.
In the future, we hope to encourage universities and other organisations to make deployed versions available for usage by the research community.


## Access Control in Amcat4 Explained

`amcat4` provides fine-grained access control, which enables administrators of an instance to share data in exactly the way they want and control what members of their team and outsiders can do on an index.
This section explains how access control works in `amcat4` and highlights which features and settings provide infrastructure for non-consumptive research.

We provide two sets of roles: one that is configred globally per user on an `amcat4` instance and one set that can be used per index.
In combination the three global (reader, writer and admin) and index roles (none, metareader, reader, writer and admin) offer twelve different role levels that are shown in the table below:

```{r}
#| eval: true
#| echo: false
library(kableExtra)
rio::import("roles.csv") |> 
  knitr::kable(format = "latex", 
               booktabs = TRUE, 
               longtable = FALSE,
               linesep = "",
               caption = "Access roles in amcat4",
               label = "tbl-roles") |> 
  kable_styling(font_size = 9,
                full_width = FALSE,
                latex_options = c("repeat_header")) |> 
  column_spec(1, width = "1.3cm") |>
  column_spec(2, width = "1.3cm") |>
  column_spec(3, width = "1.3cm") |>
  column_spec(4, width = "13cm") |> 
  kableExtra::footnote(symbol = "Relevant for non-consumptive research") |> 
  landscape()
```


Relevant for non-consumptive research are the metareader levels.
Users with this index role can perform analysis on the data via the AmCAT dashboard or the API, but do not have access to the text data.

Additionally, we can control the default role users have on a given index (see level 0 in the table).
We call this the guest role of an index.
This way, it is possible, for example, to have one index where everyone can see the metadata while other indexes on the AmCAT instance are hidden.

Users' global and index role and the guest roles of indexes can be controlled via the dashboard or the API.
We show this illustrativly in the example below.

However, first we need to highlitght another setting that can modify access to data in `amcat4`: the authentification mode.
The authentification mode of an `amcat4` instance controls who has access to the data in the first place.
Unlike the roles, this setting can only be accessed by administrators through the command line on the machine where the instance is hosted.
If you follow our recommended way of installation (see the [online manual](https://amcat.nl/book/02._getting-started.html#setup-through-docker)), `amcat4` will run in a Docker container.
When invoked with the command `docker exec -it amcat4 amcat4 config`, an interactive configuration menu will guide the user through various settings.
The authentication settings will show the follwing explanation:

```{#lst-auth lst-cap="Authentication modes in amcat4"}
auth: Do we require authorization?
  Possible choices:
  - no_auth: everyone (that can reach the server) can do anything they want
  - allow_guests: everyone can use the server, dependent on index-level 
      guest_role authorization settings
  - allow_authenticated_guests: everyone can use the server, if they have 
      a valid middlecat login, and dependent on index-level guest_role 
      authorization settings
  - authorized_users_only: only people with a valid middlecat login and
      an explicit server role can use the server

The current value for auth is AuthOptions.no_auth.
Enter a new value, press [enter] to leave unchanged, or press [control+c] 
  to abort:
```

The explanations of the different levels should hopefully be clear, except the term **middlecat login**. 
For authentication modes beyond allow_guests, people whom you want to grant some kind of access need to log in.
To make this more secure, we wrote our own authentication provider called **middlecat**.
It enables the administrator to set up authentication via different identity providers like Google or GitHub, with a fallback solution to let users log in via a one-time email link (if they want to log in again, they need to request a new link).
We host a middlecat instace at <https://middlecat.up.railway.app>, but as the software is open source, everone can set up their own instance and negotiate with different identity providers to make other authentication options available (e.g., through their university or company).

![Middlecat login screen](https://amcat.nl/book/media/amcat-2-middlecat.png){#fig-middlecat}

In cases where non-consuptive research should be made possible, all modes except *no_auth* can be used.
The indexes can then be configured to let users have a metareader role explicitly or via the guest role of an index, as explained above.

## Non-Consumptive Research via the AmCAT Dashboard: an Example

As Illustration of the access control, we can use an instance where the authentication mode is set to *allow_guests* and where we added a test index.
To reproduce this setup, one could spin up an instance of the AmCAT suite using [Docker](https://docs.docker.com/engine/install/) and [Docker Compose](https://docs.docker.com/compose/install/) (you can find a more detailed explanations in the [AmCAT manual](https://amcat.nl/book/02._getting-started.html)):

```{#lst-setup .bash lst-cap="Creating an AmCAT instance through Docker"}
# download our docker compose file with curl or manually
curl -O https://raw.githubusercontent.com/ccs-amsterdam/actioncat/main/
  actions/dfm/docker-compose.yml
# run docker compose to download and start the AmCAT applications
docker-compose up --pull="missing" -d
# create a test index to use in this example
docker exec -it amcat4 amcat4 create-test-index
# configure the instance to run in allow_guests (make sure to also set 
  your email address as admin email, or you are locked out)
docker exec -it amcat4 amcat4 config
```

We can then change the guest role of the test index through the web dashboard at <http://localhost/> (if you hosted the instance locally on your computer):

1. Log into the dashboard

![Web dashboard of AmCAT](https://amcat.nl/book/media/amcat-1.png){#fig-dashboard}

2. Select the *state_of_the_union* index and go to settings:

![Change guest role of index state_of_the_union](media/sotu_guest_role.png){#fig-cha-roles}

After this, both users with and without access to the text data are still able to perform simple frequency analyses using text queries:

![Frequency analysis](media/sotu_analysis_1.png){#fig-analysis1}

Users can also compare document groups based on meta fields:

![Comparison based on party in the state_of_the_union index](media/sotu_analysis_2.png){#fig-analysis2}

Only when users try to see more of a text than a short snippet do the index roles matter:

![User with no role](media/sotu_text_guest.png){#fig-guest}

![User with reader role](media/sotu_text_reader.png){#fig-reader}

As explained above, we could have accomplished the same by assiging a specific user either role level 2 or 7.

Through the fine-grained access control and the dashboard, AmCAT offers ways to query, visualise and analyse text data without giving users access to the underlying data.
This allows researchers to make datasets available for non-consuptive research where it is not possible to publicly share the full data due to copyright, privacy, or other concerns.

# Packaged Non-Consumptive Analysis Workflows

While `amcat4` offers rudimentary analysis methods for users with metareader access like counting and cross-tabulating documents using queries, we also developed and addon framework to AmCAT that makes it possible to run entire analysis workflows in a non-consuptive way.
We call this framework `actioncat`
The basic idea we implemented is to leverage the open source Docker infrastrucre, which we also employ to make the AmCAT suite of packages available, to let users create specialised workflows in order to perform analyses on data they do not have access to.

We offer two example *actions* (which is what we call predefined workflows that are packaged in a Docker image/container), one in `R`, one in `Python`:

- The `R` action adds a tidy document-feature representation field to an index
- The `Python` action adds a document embeddings field to an index

Both of these actions are destructive preprocessing workflows in the sense that the original text cannot be reconstructed from the new field.
This makes these actions well suited for indexes where the full text can not be shared because of copyright, privacy or other concerns.
Using AmCAT's fine grained access control features, the full text can be hidden from users without specific permissions, but the preprocessed data can still be shared with a wider audience.
Users can imitate these examples to create their own workflows and send them to administrators of an `amcat4` instance.
After approval, the administrator can then run a workflow using just two commands: one to download and one to run the action.
Compared to sending just `R` or `Python` files for processing, this approach has the advantage that the action will have all the right dependencies already and perform the action exactly as on the user's machine (therby standardizing the process to a certain degree and making the admins life a little easier).

## Non-Consumptive Research via `actioncat`: an Example

To illustrate the basic steps from the administrators perspective, we use one of the example workflows in the repository here: <https://github.com/ccs-amsterdam/actioncat>.
First, we spin up an instance of the AmCAT suite using [Docker](https://docs.docker.com/engine/install/) and [Docker Compose](https://docs.docker.com/compose/install/) as in @lst-setup.
Administrators can then use actions with the same basic approach as shown in @lst-dfm.
In case of a user defined action, the only difference for the administrator will be the link to the Docker Compose file -- which could also be sent via email, for example.
The action will download the container image, start a process and this specific one will run until it has added a tidy document-feature representation to all texts in the test index.
The user can access the results immediately as they are processed via the Web interface as shown in @fig-dfm or using the code in @lst-amcat4r to access the API.

```{#lst-dfm .bash lst-cap="Running the predefined dfm action"}
# download the action file with curl or manually
curl -O https://raw.githubusercontent.com/ccs-amsterdam/actioncat/main/
  actions/dfm/docker-compose.yml
# run the action
docker-compose up --pull="missing" -d
```

![Example text preview including dfm](https://github.com/ccs-amsterdam/actioncat/raw/main/media/dfm.png){#fig-dfm}

```{#lst-amcat4r .r lst-cap="Querying the new field through the API"}
if (!requireNamespace("amcat4r", quietly = TRUE))   
  remotes::install_github("ccs-amsterdam/amcat4r")
library(amcat4r)
amcat_login("http://localhost/amcat")
sotu_dfm <- query_documents(index = "state_of_the_union", 
                            queries = NULL, fields = c(".id", "dfm"))
sotu_dfm


    # A tibble: 232 × 2
       .id      dfm           
       <id_col> <list>        
     1 9d8…0d0  <list [3,370]>
     2 846…068  <list [2,176]>
     3 2b6…aa5  <list [2,895]>
     4 4f3…8bf  <list [3,172]>
     5 c36…4b0  <list [3,739]>
     6 5a2…8ba  <list [3,745]>
     7 8b0…5f2  <list [3,927]>
     8 484…893  <list [3,308]>
     9 a3a…a70  <list [2,554]>
    10 57c…840  <list [1,729]>
    # i 222 more rows
```

By changing the `docker-compose.yml` (shown in @lst-docker), it is possible to control which index the action is applied on, the name of the text field and the name of the new dfm field by changing the environment variables.

```{#lst-docker .r lst-cap="Querying the new field through the API"}
version: "3.8"
services:
  action-dfm:
    image: ccsamsterdam/amcat-action-dfm:4.0.13
    build: .
    network_mode: "host"    
    environment: # behaviour of the R script is controlled through these variables
      - amcat4_host=http://localhost/amcat
      - index=state_of_the_union
      - queries=NULL
      - text_field=text
      - dfm_field=dfm
    # for authentication, this container needs access to the httr2 cache directory. 
    # You can find it with `rappdirs::user_cache_dir("httr2")`
    # volumes:
    #   - ~/.cache/httr2:/root/.cache/httr2 # [local path]:[container path]
```


# Conclusion

This does not allow quite the interactive exploration that, e.g., HathiTrust's "data capsules" do, as workflows need to be pre-approved, but the framework is a lot more flexible in that any workflow can be packaged and sent to the admin of an AmCAT instance for quick and seamless review and authorization.

A crucial criterion for the infrastructure developed by OPTED is scientific transparency to facilitate replication, collaboration, and efficient re-use of academic contributions in terms of data collection and tool creation.
The features in the AmCAT suite of packages enable researchers to make data available in ways that conform with copyright, privacy, or other concerns.
Following the ideas developed within the concept of non-consumptive research, we can enable new analyses or replication effectivly without sharing access to the full material.


{{< pagebreak >}}

# References
